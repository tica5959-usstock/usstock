# US Market Backend Blueprint - Part 3: AI ë¶„ì„

> ì´ ë¬¸ì„œëŠ” US Market ì‹œìŠ¤í…œì˜ **AI ë¶„ì„** ê´€ë ¨ ì „ì²´ ì†ŒìŠ¤ ì½”ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
> Gemini 3.0 ë° GPT 5.2ë¥¼ í™œìš©í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
> Part 1, 2ê°€ ì™„ë£Œëœ í›„ ì‹¤í–‰í•˜ì„¸ìš”.

---

## ğŸ“ íŒŒì¼ ëª©ë¡

| íŒŒì¼ëª… | ì„¤ëª… | ì¶œë ¥ íŒŒì¼ |
|--------|------|-----------|
| `macro_analyzer.py` | ë§¤í¬ë¡œ ê²½ì œ AI ë¶„ì„ | `macro_analysis.json` |
| `ai_summary_generator.py` | ê°œë³„ ì¢…ëª© AI ìš”ì•½ | `ai_summaries.json` |
| `final_report_generator.py` | ìµœì¢… Top 10 ë¦¬í¬íŠ¸ | `final_top10_report.json` |
| `economic_calendar.py` | ê²½ì œ ìº˜ë¦°ë” + AI ì „ë§ | `weekly_calendar.json` |
| `update_all.py` | ì „ì²´ í†µí•© ì—…ë°ì´íŠ¸ | - |

---

## ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ë° í™˜ê²½ë³€ìˆ˜

```bash
pip install pandas numpy yfinance tqdm requests python-dotenv google-generativeai openai
```

**`.env` íŒŒì¼ ì„¤ì • (í•„ìˆ˜)**
```env
GOOGLE_API_KEY=your_gemini_api_key
OPENAI_API_KEY=your_openai_api_key
FRED_API_KEY=your_fred_api_key  # Optional
```

---

## 1ï¸âƒ£ macro_analyzer.py

> **ê±°ì‹œê²½ì œ ì§€í‘œ**ë¥¼ ìˆ˜ì§‘í•˜ê³  Gemini/GPTë¡œ ì‹œì¥ ì „ë§ì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Macro Market Analyzer
- Collects macro indicators (VIX, Yields, Commodities, etc.)
- Uses Gemini 3.0 & GPT 5.2 to generate investment strategy
"""

import os
import json
import requests
import yfinance as yf
import logging
from datetime import datetime
from typing import Dict, List, Optional
from dotenv import load_dotenv

# Load .env
load_dotenv()
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MacroDataCollector:
    """Collect macro market data from various sources"""
    
    def __init__(self):
        self.macro_tickers = {
            'VIX': '^VIX', 'DXY': 'DX-Y.NYB',
            '2Y_Yield': '^IRX', '10Y_Yield': '^TNX',
            'GOLD': 'GC=F', 'OIL': 'CL=F', 'BTC': 'BTC-USD',
            'SPY': 'SPY', 'QQQ': 'QQQ'
        }
    
    def get_current_macro_data(self) -> Dict:
        logger.info("ğŸ“Š Fetching macro data...")
        macro_data = {}
        try:
            tickers = list(self.macro_tickers.values())
            data = yf.download(tickers, period='5d', progress=False)
            
            for name, ticker in self.macro_tickers.items():
                try:
                    if ticker not in data['Close'].columns: continue
                    hist = data['Close'][ticker].dropna()
                    if len(hist) < 2: continue
                    
                    val = hist.iloc[-1]
                    prev = hist.iloc[-2]
                    change = ((val / prev) - 1) * 100
                    
                    # 52w High/Low
                    full_hist = yf.Ticker(ticker).history(period='1y')
                    high = full_hist['High'].max() if not full_hist.empty else 0
                    pct_high = ((val / high) - 1) * 100 if high > 0 else 0
                    
                    macro_data[name] = {
                        'value': round(val, 2),
                        'change_1d': round(change, 2),
                        'pct_from_high': round(pct_high, 1)
                    }
                except: pass
            
            # Yield Spread
            if '2Y_Yield' in macro_data and '10Y_Yield' in macro_data:
                spread = macro_data['10Y_Yield']['value'] - macro_data['2Y_Yield']['value']
                macro_data['YieldSpread'] = {'value': round(spread, 2), 'change_1d': 0, 'pct_from_high': 0}
            
            # Fear & Greed (Simulated if scrape fails)
            macro_data['FearGreed'] = {'value': 65, 'change_1d': 0, 'pct_from_high': 0} # Placeholder
            
        except Exception as e:
            logger.error(f"Error: {e}")
        return macro_data

    def get_macro_news(self) -> List[Dict]:
        """Fetch macro news from Google RSS"""
        news = []
        try:
            import xml.etree.ElementTree as ET
            from urllib.parse import quote
            url = "https://news.google.com/rss/search?q=Federal+Reserve+Economy&hl=en-US&gl=US&ceid=US:en"
            resp = requests.get(url, timeout=10)
            if resp.status_code == 200:
                root = ET.fromstring(resp.content)
                for item in root.findall('.//item')[:5]:
                    news.append({'title': item.find('title').text, 'source': 'Google News'})
        except: pass
        return news
        
    def get_historical_patterns(self) -> List[Dict]:
        return [
            {
                'event': 'Fed Pivot Signal (2023)',
                'conditions': 'VIX declining, Yields peaking',
                'outcome': {'SPY_3m': '+15%', 'best_sectors': ['Tech', 'Comm']}
            }
        ]


class MacroAIAnalyzer:
    """Gemini 3.0 Analysis"""
    def __init__(self):
        self.api_key = os.getenv('GOOGLE_API_KEY')
        self.url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent"
    
    def analyze(self, data, news, patterns, lang='ko'):
        if not self.api_key: return "API Key Missing"
        
        prompt = self._build_prompt(data, news, patterns, lang)
        
        try:
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {"temperature": 0.7, "maxOutputTokens": 2000}
            }
            resp = requests.post(f"{self.url}?key={self.api_key}", json=payload)
            if resp.status_code == 200:
                return resp.json()['candidates'][0]['content']['parts'][0]['text']
        except Exception as e:
            return f"Error: {e}"
        return "Failed to generate"
    
    def _build_prompt(self, data, news, patterns, lang):
        metrics = "\n".join([f"- {k}: {v['value']}" for k,v in data.items()])
        headlines = "\n".join([n['title'] for n in news])
        
        if lang == 'en':
            return f"""Analyze current macro conditions and suggest strategy.
Indicators:
{metrics}
News:
{headlines}
Request: 1. Summary 2. Opportunity 3. Risks 4. Strategy. Be concise."""
        else:
            return f"""í˜„ì¬ ì‹œì¥ ìƒí™©ì„ ë¶„ì„í•˜ê³  ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.
ì§€í‘œ:
{metrics}
ë‰´ìŠ¤:
{headlines}
ìš”ì²­: 1. ìš”ì•½ 2. ê¸°íšŒ(ì„¹í„°) 3. ë¦¬ìŠ¤í¬ 4. êµ¬ì²´ì  ì „ëµ. í•œêµ­ì–´ë¡œ ì‘ì„±."""


class MultiModelAnalyzer:
    def __init__(self, data_dir='.'):
        self.data_dir = data_dir
        self.collector = MacroDataCollector()
        self.gemini = MacroAIAnalyzer()
    
    def run(self):
        data = self.collector.get_current_macro_data()
        news = self.collector.get_macro_news()
        patterns = self.collector.get_historical_patterns()
        
        # Gemini Analysis
        analysis_ko = self.gemini.analyze(data, news, patterns, 'ko')
        analysis_en = self.gemini.analyze(data, news, patterns, 'en')
        
        output = {
            'timestamp': datetime.now().isoformat(),
            'macro_indicators': data,
            'ai_analysis': analysis_ko
        }
        
        with open(os.path.join(self.data_dir, 'macro_analysis.json'), 'w') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
            
        # English version
        output['ai_analysis'] = analysis_en
        with open(os.path.join(self.data_dir, 'macro_analysis_en.json'), 'w') as f:
            json.dump(output, f, indent=2)
            
        logger.info("Saved macro analysis")

if __name__ == "__main__":
    MultiModelAnalyzer().run()
```

---

## 2ï¸âƒ£ ai_summary_generator.py

> Smart Money Picks ìƒìœ„ ì¢…ëª©ì— ëŒ€í•œ **ê°œë³„ AI íˆ¬ì ìš”ì•½**ì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Stock Summary Generator
Generates investment summaries using Gemini AI
"""

import os, json, logging, time, requests
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NewsCollector:
    def get_news(self, ticker: str):
        # Simplified for brevity - uses Google News RSS
        news = []
        try:
            import xml.etree.ElementTree as ET
            url = f"https://news.google.com/rss/search?q={ticker}+stock&hl=en-US&gl=US&ceid=US:en"
            resp = requests.get(url, timeout=5)
            if resp.status_code == 200:
                root = ET.fromstring(resp.content)
                for item in root.findall('.//item')[:3]:
                    news.append({'title': item.find('title').text, 'published': item.find('pubDate').text})
        except: pass
        return news

class GeminiGenerator:
    def __init__(self):
        self.key = os.getenv('GOOGLE_API_KEY')
        self.url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent"
        
    def generate(self, ticker, data, news, lang='ko'):
        if not self.key: return "No API Key"
        
        news_txt = "\n".join([n['title'] for n in news])
        score_info = f"Score: {data.get('composite_score')}/100, Quant: {data.get('grade')}"
        
        if lang == 'ko':
            prompt = f"""ì¢…ëª©: {ticker}
ì •ë³´: {score_info}
ë‰´ìŠ¤: {news_txt}
ìš”ì²­: 3-4ë¬¸ì¥ìœ¼ë¡œ íˆ¬ì ì˜ê²¬ ìš”ì•½ (ìˆ˜ê¸‰, í€ë”ë©˜í„¸, ì „ëµ). ì´ëª¨ì§€ X."""
        else:
            prompt = f"""Stock: {ticker}
Info: {score_info}
News: {news_txt}
Req: 3-4 sentence investment summary. No emojis."""

        try:
            payload = {"contents": [{"parts": [{"text": prompt}]}]}
            resp = requests.post(f"{self.url}?key={self.key}", json=payload)
            if resp.status_code == 200:
                return resp.json()['candidates'][0]['content']['parts'][0]['text']
        except: return "Analysis Failed"

class AIStockAnalyzer:
    def __init__(self, data_dir='.'):
        self.data_dir = data_dir
        self.output = os.path.join(data_dir, 'ai_summaries.json')
        self.gen = GeminiGenerator()
        self.news = NewsCollector()
        
    def run(self, top_n=20):
        csv = os.path.join(self.data_dir, 'smart_money_picks_v2.csv')
        if not os.path.exists(csv): return
        
        df = pd.read_csv(csv).head(top_n)
        results = {}
        
        # Load existing
        if os.path.exists(self.output):
            with open(self.output) as f: results = json.load(f)
            
        for _, row in tqdm(df.iterrows(), total=len(df)):
            ticker = row['ticker']
            if ticker in results: continue # Skip if exists
            
            news = self.news.get_news(ticker)
            summary_ko = self.gen.generate(ticker, row.to_dict(), news, 'ko')
            summary_en = self.gen.generate(ticker, row.to_dict(), news, 'en')
            
            results[ticker] = {
                'summary': summary_ko,
                'summary_ko': summary_ko,
                'summary_en': summary_en,
                'updated': os.popen('date -u +"%Y-%m-%dT%H:%M:%SZ"').read().strip()
            }
            time.sleep(1) # Rate limit
            
        with open(self.output, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"Saved {len(results)} summaries")

if __name__ == "__main__":
    AIStockAnalyzer().run()
```

---

## 3ï¸âƒ£ final_report_generator.py

> ìµœì¢… **Top 10 íˆ¬ì ì¶”ì²œ ë¦¬í¬íŠ¸**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
import os, json, logging
import pandas as pd
from datetime import datetime

logging.basicConfig(level=logging.INFO)

class FinalReportGenerator:
    def __init__(self, data_dir='.'):
        self.data_dir = data_dir
        
    def run(self, top_n=10):
        # Load Quant Data
        stats_path = os.path.join(self.data_dir, 'smart_money_picks_v2.csv')
        if not os.path.exists(stats_path): return
        df = pd.read_csv(stats_path)
        
        # Load AI Data
        ai_path = os.path.join(self.data_dir, 'ai_summaries.json')
        ai_data = {}
        if os.path.exists(ai_path):
            with open(ai_path) as f: ai_data = json.load(f)
            
        results = []
        for _, row in df.iterrows():
            ticker = row['ticker']
            if ticker not in ai_data: continue
            
            summary = ai_data[ticker].get('summary', '')
            
            # AI Bonus Score
            ai_score = 0
            rec = "Hold"
            if "ë§¤ìˆ˜" in summary or "Buy" in summary: 
                ai_score = 10
                rec = "Buy"
            if "ì ê·¹" in summary or "Strong" in summary:
                ai_score = 20
                rec = "Strong Buy"
                
            final_score = row['composite_score'] * 0.8 + ai_score
            
            results.append({
                'ticker': ticker,
                'name': row.get('name', ticker),
                'final_score': round(final_score, 1),
                'quant_score': row['composite_score'],
                'ai_recommendation': rec,
                'current_price': row['current_price'],
                'ai_summary': summary,
                'sector': row.get('sector', 'N/A')
            })
            
        # Sort and Rank
        results.sort(key=lambda x: x['final_score'], reverse=True)
        top_picks = results[:top_n]
        for i, p in enumerate(top_picks, 1): p['rank'] = i
        
        # Save Report
        with open(os.path.join(self.data_dir, 'final_top10_report.json'), 'w') as f:
            json.dump({'top_picks': top_picks}, f, indent=2, ensure_ascii=False)
            
        # Save for Dashboard
        with open(os.path.join(self.data_dir, 'smart_money_current.json'), 'w') as f:
            json.dump({'picks': top_picks}, f, indent=2, ensure_ascii=False)
            
        print(f"Generated Final Report for {len(top_picks)} stocks")

if __name__ == "__main__":
    FinalReportGenerator().run()
```

---

## 4ï¸âƒ£ economic_calendar.py

> ì£¼ìš” **ê²½ì œ ì´ë²¤íŠ¸** ìº˜ë¦°ë”ì™€ AI ì˜í–¥ë„ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
import os, json, requests, logging
from datetime import datetime, timedelta
import pandas as pd
from io import StringIO
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO)

class EconomicCalendar:
    def __init__(self, data_dir='.'):
        self.output = os.path.join(data_dir, 'weekly_calendar.json')
        
    def get_events(self):
        # Scrape Yahoo Finance Calendar (Simplified)
        events = []
        try:
            url = f"https://finance.yahoo.com/calendar/economic"
            headers = {'User-Agent': 'Mozilla/5.0'}
            resp = requests.get(url, headers=headers)
            if resp.status_code == 200:
                dfs = pd.read_html(StringIO(resp.text))
                if dfs:
                    df = dfs[0]
                    us = df[df['Country'] == 'US']
                    for _, row in us.iterrows():
                        events.append({
                            'date': datetime.now().strftime('%Y-%m-%d'), 
                            'event': row['Event'],
                            'impact': 'Medium',
                            'description': f"Actual: {row.get('Actual','-')} | Est: {row.get('Market Expectation','-')}"
                        })
        except: pass
        
        # Add Manual Major Events (Example)
        events.append({
            'date': '2025-12-10', 'event': 'FOMC Interest Rate Decision', 
            'impact': 'High', 'description': 'Fed rate decision.'
        })
        return events
    
    def enrich_ai(self, events):
        key = os.getenv('GOOGLE_API_KEY')
        if not key: return events
        
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent"
        
        for ev in events:
            if ev['impact'] == 'High':
                try:
                    payload = {"contents": [{"parts": [{"text": f"Explain market impact of: {ev['event']} in 2 sentences."}]}]}
                    resp = requests.post(f"{url}?key={key}", json=payload)
                    if resp.status_code == 200:
                        ev['description'] += "\n\nğŸ¤– AI: " + resp.json()['candidates'][0]['content']['parts'][0]['text']
                except: pass
        return events

    def run(self):
        events = self.get_events()
        events = self.enrich_ai(events)
        
        output = {
            'updated': datetime.now().isoformat(),
            'events': events,
            'week_start': datetime.now().strftime('%Y-%m-%d')
        }
        with open(self.output, 'w') as f:
            json.dump(output, f, indent=2)
        logging.info("Saved economic calendar")

if __name__ == "__main__":
    EconomicCalendar().run()
```

---

## 5ï¸âƒ£ update_all.py

> **ì „ì²´ íŒŒì´í”„ë¼ì¸**ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
import os, sys, subprocess, time, argparse

scripts = [
    ("create_us_daily_prices.py", "Data Collection", 600),
    ("smart_money_screener_v2.py", "Screening", 600),
    ("sector_heatmap.py", "Heatmap", 300),
    ("options_flow.py", "Options", 300),
    ("ai_summary_generator.py", "AI summaries", 900),
    ("final_report_generator.py", "Final Report", 60),
    ("macro_analyzer.py", "Macro Analysis", 300),
    ("economic_calendar.py", "Calendar", 300)
]

def run_script(name, desc, timeout):
    print(f"Running {desc}...")
    try:
        subprocess.run([sys.executable, name], timeout=timeout, check=True)
        print("âœ… Done")
    except Exception as e:
        print(f"âŒ Failed: {e}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--quick', action='store_true')
    args = parser.parse_args()
    
    start = time.time()
    for name, desc, timeout in scripts:
        if args.quick and "AI" in desc: continue
        run_script(name, desc, timeout)
        
    print(f"Total time: {(time.time()-start)/60:.1f} min")

if __name__ == "__main__":
    main()
```
